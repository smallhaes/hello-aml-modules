{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.pipeline.wrapper import Module, Pipeline\n",
    "workspace = Workspace.from_config()\n",
    "print(workspace.name, workspace.resource_group, workspace.location, workspace.subscription_id, sep='\\n')\n",
    "\n",
    "aml_compute_target = \"aml-compute\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(workspace, aml_compute_target)\n",
    "    print(\"Found existing compute target: {}\".format(aml_compute_target))\n",
    "except:\n",
    "    print(\"Creating new compute target: {}\".format(aml_compute_target))\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                min_nodes = 1, \n",
    "                                                                max_nodes = 4)    \n",
    "    aml_compute = ComputeTarget.create(workspace, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "try:\n",
    "    mpi_train_module_func = Module.load(workspace, namespace=\"microsoft.com/azureml/samples\", name=\"Hello World MPI Job\")\n",
    "except:\n",
    "    mpi_train_module_func = Module.register(workspace, os.path.join('modules', 'mpi_module', 'module_spec.yaml'))\n",
    "\n",
    "from azureml.pipeline.wrapper._dataset import get_global_dataset_by_path\n",
    "blob_input_data = get_global_dataset_by_path(workspace, 'Automobile_price_data', 'GenericCSV/Automobile_price_data_(Raw)')\n",
    "\n",
    "mpi_train = mpi_train_module_func(input_path = blob_input_data, string_parameter = \"test1\")\n",
    "mpi_train.runsettings.configure(node_count=2, process_count_per_node=2)\n",
    "\n",
    "print(mpi_train.runsettings.node_count)\n",
    "mpi_train.runsettings.node_count = 1\n",
    "\n",
    "test_pipeline = Pipeline(nodes=[mpi_train], name=\"test mpi\", default_compute_target='aml-compute')\n",
    "test_pipeline.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.pipeline.wrapper import Module, dsl\n",
    "from azureml.pipeline.wrapper._dataset import get_global_dataset_by_path\n",
    "from external_sub_pipeline import external_sub_pipeline0\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')\n",
    "\n",
    "# Module\n",
    "execute_python_script_module = Module.load(ws, namespace='azureml', name='Execute Python Script')\n",
    "\n",
    "\n",
    "# TODO: Dataset\n",
    "blob_input_data = get_global_dataset_by_path(ws, 'Automobile_price_data', 'GenericCSV/Automobile_price_data_(Raw)')\n",
    "\n",
    "training_data_name = 'aml_module_training_data'\n",
    "\n",
    "if training_data_name not in ws.datasets:\n",
    "    print('Registering a training dataset for sample pipeline ...')\n",
    "    train_data = Dataset.File.from_files(path=['https://dprepdata.blob.core.windows.net/demo/Titanic.csv'])\n",
    "    train_data.register(workspace=ws,\n",
    "                        name=training_data_name,\n",
    "                        description='Training data (just for illustrative purpose)')\n",
    "    print('Registerd')\n",
    "else:\n",
    "    train_data = ws.datasets[training_data_name]\n",
    "    print('Training dataset found in workspace')\n",
    "    \n",
    "\n",
    "@dsl.pipeline(name='sub0 graph', description='sub0')\n",
    "def sub_pipeline0(input):\n",
    "    module1 = execute_python_script_module(\n",
    "        # should be pipeline input\n",
    "        dataset1=input,\n",
    "    )\n",
    "    module2 = execute_python_script_module(\n",
    "        dataset1=module1.outputs.result_dataset,\n",
    "    )\n",
    "    return module2.outputs\n",
    "\n",
    "\n",
    "@dsl.pipeline(name='sub1 graph', description='sub1')\n",
    "def sub_pipeline1(input):\n",
    "    module1 = execute_python_script_module(\n",
    "        dataset1=input\n",
    "    )\n",
    "    sub0 = sub_pipeline0(module1.outputs.result_dataset)\n",
    "    return sub0.outputs\n",
    "\n",
    "\n",
    "@dsl.pipeline(name='sub2 graph', description='sub1')\n",
    "def sub_pipeline2(input):\n",
    "    module1 = execute_python_script_module(\n",
    "        dataset1=input\n",
    "    )\n",
    "    module2 = execute_python_script_module(\n",
    "        dataset1=module1.outputs.result_dataset,\n",
    "        dataset2=blob_input_data\n",
    "    )\n",
    "    module3 = execute_python_script_module(\n",
    "        dataset1=input,\n",
    "        dataset2=module2.outputs.result_dataset\n",
    "    )\n",
    "    module4 = execute_python_script_module(\n",
    "        dataset1=train_data,\n",
    "        dataset2=module3.outputs.result_dataset\n",
    "    )\n",
    "    sub0 = sub_pipeline0(module4.outputs.result_dataset)\n",
    "    return sub0.outputs\n",
    "\n",
    "\n",
    "@dsl.pipeline(name='parent graph', description='parent', default_compute_target=\"aml-compute\")\n",
    "def parent_pipeline():\n",
    "    @dsl.pipeline(name='internal sub graph', description='internal sub')\n",
    "    def sub_pipeline_internal(input):\n",
    "        module1 = execute_python_script_module(\n",
    "            # should be pipeline input\n",
    "            dataset1=input,\n",
    "        )\n",
    "        module2 = execute_python_script_module(\n",
    "            dataset1=module1.outputs.result_dataset,\n",
    "        )\n",
    "        return module2.outputs\n",
    "\n",
    "    sub0 = sub_pipeline_internal(blob_input_data)\n",
    "    sub1 = sub_pipeline1(sub0.outputs.result_dataset)\n",
    "    sub2 = sub_pipeline2(sub1.outputs.result_dataset)\n",
    "    module2 = execute_python_script_module(\n",
    "        dataset1=sub2.outputs.result_dataset,\n",
    "        dataset2=train_data,\n",
    "    )\n",
    "\n",
    "    external = external_sub_pipeline0(sub1.outputs.result_dataset)\n",
    "    return module2.outputs\n",
    "\n",
    "pipeline1 = parent_pipeline()\n",
    "pipeline1.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1.diff(test_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline.diff(pipeline1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aml)",
   "language": "python",
   "name": "aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
